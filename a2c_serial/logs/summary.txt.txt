r0 : [reward = -|cos(theta_1)|]
r1 : [reward = |sin(theta_1)|]
r2 : [reward = 1/|cos(theta1)+0.1|-1/(1+0.1)]
r3 : [reward = -cos(2*theta_1)]


v1 : 초기 환경
v2 : 실제 환경
v3 : 클래스 기반 환경
v4 : 텐서 기반 환경


Acrobot-v2_0826_19-31-03_r1_1e-3	r1버전 학습 실패
Acrobot-v2_0828_15-29-22_r0_1e-3	r0버전 학습 실패
Acrobot-v2_0829_11-55-21_r2_1e-3	r2버전 학습 실패
Acrobot-v2_0829_21-22-55_r1_1e-4	학습률 변경 후 학습 실패
Acrobot-v3_0830_14-36-46_r1_1e-3	코드 마이너 수정 후(v3) action1로 수렴하여 실패
Acrobot-v3_0830_17-14-36_r1_1e-3	코드 마이너 수정 후 더 빠르게 action0로 수렴하여 실패
Acrobot-v4_0910_03-47-54_r1_1e-3	텐서 기반으로 코드 전면 개편(v4) 후 action loss를 1/1000으로 스케일링 하여 학습, action0로 수렴하여 학습 실패
Acrobot-v4_0913_14-17-50_r3_1e-4	새로운 리워드(r3)를 정의하여 학습 후 action0로 수렴하여 실패
Acrobot-v4_0914_13-04-40_r3_1e-5	r3로 학습률을 더 낮춰 재학습 하였더니 수렴하지 않아 실패
Acrobot-v4_0915_20-03-36_r1_15e-5	r1으로 리워드 변경하고 1e-5(수렴안함)와 1e-4(수렴함)의 중간 학습률로 재학습 후, action0로 수렴하여 실패
Acrobot-v4_0916_14-16-50_r1_12e-5	성공적으로 학습이 진행되었으나 그네 안정화를 위한 에피소드간 딜레이가 맞지 않아 중단(지금까지 높이 올라간 적이 없어 몰랐음)